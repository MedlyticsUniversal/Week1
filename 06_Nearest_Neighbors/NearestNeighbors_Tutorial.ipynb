{"cells":[{"cell_type":"markdown","metadata":{"id":"S36f2M-S6bsV","colab_type":"text","cell_id":"d4272dd3629f40eb80b9bb1c46c9652e","deepnote_cell_type":"markdown"},"source":"# Nearest Neighbors Tutorial","block_group":"00000-c9c7686c-f64a-4c7e-a620-ee71538a7bd1"},{"cell_type":"markdown","metadata":{"id":"Xb0q8y0rq6Ve","colab_type":"text","cell_id":"302fb91cfde54eb58a4051225afd5ec1","deepnote_cell_type":"markdown"},"source":"The basic concept for **Nearest Neighbors Algorithms** (NNAs) is to classify a datum by finding one or more points that have similar features. The most similar points are called the **nearest neighbors**. Once found, the input datum can be put in the same class as its neighbors. We can also use this to predict the value of missing features for the input datum.","block_group":"00001-0b5dbf8c-0ec9-4f9e-9980-e7ec9d102b7b"},{"cell_type":"markdown","metadata":{"id":"PdwUJygSGU9F","colab_type":"text","cell_id":"71f05a8104984506a38c4efa1fa237e9","deepnote_cell_type":"markdown"},"source":"## K-Nearest Neighbors\n\nThe most commonly used NNA is **k-Nearest Neighbors,** in which the top $k$ nearest neighbors (best matches) are identified. In most instantiations of KNN, classification or prediction is based on a **majority vote** of the $k$ nearest neighbors.\n> For example, if $k = 5$ and at least 3 out of the 5 nearest neighbors of an input datum are class A, then we would assign the new datum to class A.\n\nFor a more complex example, see the image below. Here, if $k = 1$, the green circle would be assigned to Class 1, since the nearest point is a blue square. However, if $k = 3$, the answer becomes Class 2, since the next two closest are both red triangles.\n\n\n![knn.png](https://raw.githubusercontent.com/MedlyticsUniversal/Data/main/Images/knn.png)","block_group":"00002-e4dc9a5f-f11b-4f08-b1a5-db405ef985cc"},{"cell_type":"markdown","metadata":{"id":"iZrldzQgy9ud","colab_type":"text","cell_id":"f9a89571d94f4eddb0c99c57af2d9622","deepnote_cell_type":"markdown"},"source":"## Quick example to illustrate a KNN:\n\nIn this section, we will use sklearn to build a k-NNA/KNN model for a dataset describing cars. Our goal will be to classify the cars into one of two categories: \"cool\" or \"uncool.\" Clearly these are subjective terms, but that's okay: we will provide a manually classified training set.\n\nFor example, if we consider the variables _horsepower, number of seats,_ and _auto_ (manual = 0, automatic = 1), our manually classified training set might look like this:\n\n- 150, 5, 0, uncool (2008 Honda Civic)  \n- 320, 5, 0, cool (2011 Dodge Charger)\n- 383, 3, 1, cool (1985 Chevy Blazer)\n- 210, 7, 0, uncool (2001 Honda Odyssey)\n\nLet's say we're trying to predict whether the 2017 Bugatti Veyron (1500hp, 2 seats, auto = 1) is cool or not. Our first step is to load the data into a Python structure.","block_group":"00003-bcf7a250-ee2f-424a-b415-d4caab0f228d"},{"cell_type":"code","metadata":{"id":"p64sH6r76vWE","colab":{},"colab_type":"code","source_hash":"b7dcc9c8","execution_start":1694563681698,"execution_millis":8,"deepnote_to_be_reexecuted":false,"cell_id":"89617bffab4544598f48d20696d45a8f","deepnote_cell_type":"code"},"source":"import numpy as np\nimport pandas as pd","block_group":"00004-88d8854d-8362-4428-a374-cef2e3c0e8b0","execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wX6Uc4e-61NI","colab_type":"text","cell_id":"eb928e1cd2e74806b83033d2e77a7481","deepnote_cell_type":"markdown"},"source":"### Loading the data","block_group":"00005-79d68226-5f16-45ff-b694-875e94a13209"},{"cell_type":"code","metadata":{"id":"s4VMsygjy9ue","colab":{"height":206,"base_uri":"https://localhost:8080/"},"outputId":"4b0d7029-00a0-4c7f-e807-b1a9cd19d3d5","colab_type":"code","source_hash":"d1409e7","execution_start":1694563734267,"execution_millis":215,"deepnote_to_be_reexecuted":false,"cell_id":"e03eaeeeb33840c2b0059b53decbd6d1","deepnote_cell_type":"code"},"source":"cars_dict = {'2008 Honda Civic':    {'hp':150., 'seats':5., 'auto':0., 'cool':0}, \n             '2011 Dodge Charger' : {'hp':320., 'seats':5., 'auto':0., 'cool':1}, \n             '1985 Chevy Blazer':   {'hp':383., 'seats':3., 'auto':1., 'cool':1}, \n             '2001 Honda Odyssey':  {'hp':210., 'seats':7., 'auto':0., 'cool':0}, \n             '2017 Bugatti Veyron': {'hp':1500.,'seats':2., 'auto':1., 'cool':None}}\n\ndata = pd.DataFrame.from_dict(cars_dict,orient='index')\ndata","block_group":"00006-bb07261b-8db9-405b-a2b3-dfab9e76ffe2","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"application/vnd.deepnote.dataframe.v3+json":{"column_count":4,"row_count":5,"columns":[{"name":"hp","dtype":"float64","stats":{"unique_count":5,"nan_count":0,"min":"150.0","max":"1500.0","histogram":[{"bin_start":150,"bin_end":285,"count":2},{"bin_start":285,"bin_end":420,"count":2},{"bin_start":420,"bin_end":555,"count":0},{"bin_start":555,"bin_end":690,"count":0},{"bin_start":690,"bin_end":825,"count":0},{"bin_start":825,"bin_end":960,"count":0},{"bin_start":960,"bin_end":1095,"count":0},{"bin_start":1095,"bin_end":1230,"count":0},{"bin_start":1230,"bin_end":1365,"count":0},{"bin_start":1365,"bin_end":1500,"count":1}]}},{"name":"seats","dtype":"float64","stats":{"unique_count":4,"nan_count":0,"min":"2.0","max":"7.0","histogram":[{"bin_start":2,"bin_end":2.5,"count":1},{"bin_start":2.5,"bin_end":3,"count":0},{"bin_start":3,"bin_end":3.5,"count":1},{"bin_start":3.5,"bin_end":4,"count":0},{"bin_start":4,"bin_end":4.5,"count":0},{"bin_start":4.5,"bin_end":5,"count":0},{"bin_start":5,"bin_end":5.5,"count":2},{"bin_start":5.5,"bin_end":6,"count":0},{"bin_start":6,"bin_end":6.5,"count":0},{"bin_start":6.5,"bin_end":7,"count":1}]}},{"name":"auto","dtype":"float64","stats":{"unique_count":2,"nan_count":0,"min":"0.0","max":"1.0","histogram":[{"bin_start":0,"bin_end":0.1,"count":3},{"bin_start":0.1,"bin_end":0.2,"count":0},{"bin_start":0.2,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0},{"bin_start":0.5,"bin_end":0.6000000000000001,"count":0},{"bin_start":0.6000000000000001,"bin_end":0.7000000000000001,"count":0},{"bin_start":0.7000000000000001,"bin_end":0.8,"count":0},{"bin_start":0.8,"bin_end":0.9,"count":0},{"bin_start":0.9,"bin_end":1,"count":2}]}},{"name":"cool","dtype":"float64","stats":{"unique_count":2,"nan_count":1,"min":"0.0","max":"1.0","histogram":[{"bin_start":0,"bin_end":0.1,"count":2},{"bin_start":0.1,"bin_end":0.2,"count":0},{"bin_start":0.2,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0},{"bin_start":0.5,"bin_end":0.6000000000000001,"count":0},{"bin_start":0.6000000000000001,"bin_end":0.7000000000000001,"count":0},{"bin_start":0.7000000000000001,"bin_end":0.8,"count":0},{"bin_start":0.8,"bin_end":0.9,"count":0},{"bin_start":0.9,"bin_end":1,"count":2}]}},{"name":"_deepnote_index_column","dtype":"object"}],"rows":[{"hp":150,"seats":5,"auto":0,"cool":0,"_deepnote_index_column":"2008 Honda Civic"},{"hp":320,"seats":5,"auto":0,"cool":1,"_deepnote_index_column":"2011 Dodge Charger"},{"hp":383,"seats":3,"auto":1,"cool":1,"_deepnote_index_column":"1985 Chevy Blazer"},{"hp":210,"seats":7,"auto":0,"cool":0,"_deepnote_index_column":"2001 Honda Odyssey"},{"hp":1500,"seats":2,"auto":1,"cool":"nan","_deepnote_index_column":"2017 Bugatti Veyron"}]},"text/plain":"                         hp  seats  auto  cool\n2008 Honda Civic      150.0    5.0   0.0   0.0\n2011 Dodge Charger    320.0    5.0   0.0   1.0\n1985 Chevy Blazer     383.0    3.0   1.0   1.0\n2001 Honda Odyssey    210.0    7.0   0.0   0.0\n2017 Bugatti Veyron  1500.0    2.0   1.0   NaN","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>hp</th>\n      <th>seats</th>\n      <th>auto</th>\n      <th>cool</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2008 Honda Civic</th>\n      <td>150.0</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2011 Dodge Charger</th>\n      <td>320.0</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1985 Chevy Blazer</th>\n      <td>383.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2001 Honda Odyssey</th>\n      <td>210.0</td>\n      <td>7.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017 Bugatti Veyron</th>\n      <td>1500.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"ezEwHH-x67cj","colab_type":"text","cell_id":"b933e42e5a1c4ae3bb1bdc82ab5e2126","deepnote_cell_type":"markdown"},"source":"### Normalizing data and calculating distances","block_group":"00007-d4a14cd5-9c3b-4fca-bc8a-6098b98a5721"},{"cell_type":"markdown","metadata":{"id":"5STDuS1DF0PA","colab_type":"text","cell_id":"bc1eb11d32ca4d949a2d22a0390d813e","deepnote_cell_type":"markdown"},"source":"To determine the class of our Bugatti using NNA, we need to measure its \"nearness\" to other cars. One of the simplest metrics for this is the Euclidian distance, defined as the square root of the sum of the differences between each feature squared:\n\n#### $\\sqrt{(q_1-p_1)^2+(q_2-p_2)^2+ \\ldots +(q_n-p_n)^2}$\n#### $= \\sqrt{\\displaystyle\\sum_{i=1}^{n} (q_i-p_i)^2 }$.\n\nSome other approaches include Chi square distance and cosine distance. For further reading on distance functions, see this article: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4978658/.\n\nNote that **by using Euclidian distance, we will be normalizing the data in a way that assumes all features are equally important**. Otherwise, values that are generally larger (such as horsepower) will end up having a larger impact on the distance. Unless we have some reason to believe that the larger features are more important, we want to balance features by **normalizing the data.** This will be helpful when we compare measurements of different units.","block_group":"00008-f0459498-b841-4f4d-a7d9-953cf333a8f5"},{"cell_type":"markdown","metadata":{"cell_id":"b7dd764064614b37963aaaa3d04016b2","deepnote_cell_type":"markdown"},"source":"#### Min-Max Normalization\n\nOne way to normalize data is to rescale the range of data to [0, 1] (or [-1, 1]). To do this, we subtract the minimum value of a feature from each of its instances and then divide them by the range of the feature values.\n\n### $ x_{scaled} = \\dfrac {x - x_{min}} {x_{max}-x_{min}} $\n\nLet's do this for our dataset:","block_group":"b7dd764064614b37963aaaa3d04016b2"},{"cell_type":"code","metadata":{"id":"aBhWCPsFMn-C","colab":{"height":206,"base_uri":"https://localhost:8080/"},"outputId":"1eccad32-88f6-4b41-d201-cb94e764dccc","colab_type":"code","source_hash":"15312311","execution_start":1694563737473,"execution_millis":237,"deepnote_to_be_reexecuted":false,"cell_id":"1748affed1014cb2be68ab17f1a646e2","deepnote_cell_type":"code"},"source":"# Normalizing each value by subtracting the minimum value in its row and then dividing by the range (max - min)\n\ndata_mmnorm = data\n\nfor i in ['hp','seats','auto']: # Remember that we don't normalize the label\n    data_mmnorm[i] = (data[i] - min(data[i].values))/(max(data[i].values) - min(data[i].values))\n    \ndata_mmnorm","block_group":"00009-a9b72de4-c24a-4204-92ed-0f2400a2682d","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"application/vnd.deepnote.dataframe.v3+json":{"column_count":4,"row_count":5,"columns":[{"name":"hp","dtype":"float64","stats":{"unique_count":5,"nan_count":0,"min":"0.0","max":"1.0","histogram":[{"bin_start":0,"bin_end":0.1,"count":2},{"bin_start":0.1,"bin_end":0.2,"count":2},{"bin_start":0.2,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0},{"bin_start":0.5,"bin_end":0.6000000000000001,"count":0},{"bin_start":0.6000000000000001,"bin_end":0.7000000000000001,"count":0},{"bin_start":0.7000000000000001,"bin_end":0.8,"count":0},{"bin_start":0.8,"bin_end":0.9,"count":0},{"bin_start":0.9,"bin_end":1,"count":1}]}},{"name":"seats","dtype":"float64","stats":{"unique_count":4,"nan_count":0,"min":"0.0","max":"1.0","histogram":[{"bin_start":0,"bin_end":0.1,"count":1},{"bin_start":0.1,"bin_end":0.2,"count":0},{"bin_start":0.2,"bin_end":0.30000000000000004,"count":1},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0},{"bin_start":0.5,"bin_end":0.6000000000000001,"count":2},{"bin_start":0.6000000000000001,"bin_end":0.7000000000000001,"count":0},{"bin_start":0.7000000000000001,"bin_end":0.8,"count":0},{"bin_start":0.8,"bin_end":0.9,"count":0},{"bin_start":0.9,"bin_end":1,"count":1}]}},{"name":"auto","dtype":"float64","stats":{"unique_count":2,"nan_count":0,"min":"0.0","max":"1.0","histogram":[{"bin_start":0,"bin_end":0.1,"count":3},{"bin_start":0.1,"bin_end":0.2,"count":0},{"bin_start":0.2,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0},{"bin_start":0.5,"bin_end":0.6000000000000001,"count":0},{"bin_start":0.6000000000000001,"bin_end":0.7000000000000001,"count":0},{"bin_start":0.7000000000000001,"bin_end":0.8,"count":0},{"bin_start":0.8,"bin_end":0.9,"count":0},{"bin_start":0.9,"bin_end":1,"count":2}]}},{"name":"cool","dtype":"float64","stats":{"unique_count":2,"nan_count":1,"min":"0.0","max":"1.0","histogram":[{"bin_start":0,"bin_end":0.1,"count":2},{"bin_start":0.1,"bin_end":0.2,"count":0},{"bin_start":0.2,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0},{"bin_start":0.5,"bin_end":0.6000000000000001,"count":0},{"bin_start":0.6000000000000001,"bin_end":0.7000000000000001,"count":0},{"bin_start":0.7000000000000001,"bin_end":0.8,"count":0},{"bin_start":0.8,"bin_end":0.9,"count":0},{"bin_start":0.9,"bin_end":1,"count":2}]}},{"name":"_deepnote_index_column","dtype":"object"}],"rows":[{"hp":0,"seats":0.6,"auto":0,"cool":0,"_deepnote_index_column":"2008 Honda Civic"},{"hp":0.1259259259259259,"seats":0.6,"auto":0,"cool":1,"_deepnote_index_column":"2011 Dodge Charger"},{"hp":0.1725925925925926,"seats":0.2,"auto":1,"cool":1,"_deepnote_index_column":"1985 Chevy Blazer"},{"hp":0.044444444444444446,"seats":1,"auto":0,"cool":0,"_deepnote_index_column":"2001 Honda Odyssey"},{"hp":1,"seats":0,"auto":1,"cool":"nan","_deepnote_index_column":"2017 Bugatti Veyron"}]},"text/plain":"                           hp  seats  auto  cool\n2008 Honda Civic     0.000000    0.6   0.0   0.0\n2011 Dodge Charger   0.125926    0.6   0.0   1.0\n1985 Chevy Blazer    0.172593    0.2   1.0   1.0\n2001 Honda Odyssey   0.044444    1.0   0.0   0.0\n2017 Bugatti Veyron  1.000000    0.0   1.0   NaN","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>hp</th>\n      <th>seats</th>\n      <th>auto</th>\n      <th>cool</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2008 Honda Civic</th>\n      <td>0.000000</td>\n      <td>0.6</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2011 Dodge Charger</th>\n      <td>0.125926</td>\n      <td>0.6</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1985 Chevy Blazer</th>\n      <td>0.172593</td>\n      <td>0.2</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2001 Honda Odyssey</th>\n      <td>0.044444</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017 Bugatti Veyron</th>\n      <td>1.000000</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","metadata":{"cell_id":"fbbd13339bf64516afab50fcd53188aa","deepnote_cell_type":"markdown"},"source":"However, a significant downside to min-max normalization is that it does not handle _outliers_ very well. For instance, normalizing the houses data below fixed the squished y-axis but was no good for the x-axis.\n\n![min_max_problem.png](https://raw.githubusercontent.com/MedlyticsUniversal/Data/main/Images/min_max_problem.png)","block_group":"fbbd13339bf64516afab50fcd53188aa"},{"cell_type":"markdown","metadata":{"cell_id":"9637b05e1ecf45e7800cf78fe5913e03","deepnote_cell_type":"markdown"},"source":"#### Z-Score Normalization (Standardization)\n\nZ-score normalization, also known as standardization, is a way of normalizing data that avoids this outlier issue. The strategy is to scale the values of a feature to have a mean of 0 and a standard deviation of 1:\n\n### $ x_{scaled} = \\dfrac {x - \\mu} {\\sigma} $\nwhere $\\mu$ is the mean value of the feature and $\\sigma$ is the standard deviation of the feature.\n\nIf a value is equal to the mean of a feature, it will be scaled to 0. If it is below the mean, it will become a negative value, and if it is above the mean, it will be a positive value. Let's try this on the houses data from before.\n\nSee how the normalized data is less squished now? Also notice that the scales of the x-axis and y-axis are nearly the same—almost all points range from -2 to 2 on both axes. The only downside to note is that features won't be on the _exact_ same scale.\n\n![z_score_norm.png](https://raw.githubusercontent.com/MedlyticsUniversal/Data/main/Images/z_score_norm.png)","block_group":"9637b05e1ecf45e7800cf78fe5913e03"},{"cell_type":"markdown","metadata":{"id":"JXcfmnfzy9u0","colab_type":"text","cell_id":"cf15ec4d47804289a167e893913d8ab5","deepnote_cell_type":"markdown"},"source":"As usual, a tool already exists to do this. Using sklearn, we can normalize data with an object called a `StandardScaler`. We will use this in the example below, but feel free to also read the documentation here:\n\nhttp://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html.","block_group":"00010-da500647-9eb3-401a-8a81-4b47ca059d73"},{"cell_type":"code","metadata":{"id":"V6hK0zn1y9u2","colab":{"height":206,"base_uri":"https://localhost:8080/"},"outputId":"9907f9a1-3dba-411b-f014-1cade5571b6d","colab_type":"code","source_hash":"92898f69","execution_start":1694563740850,"execution_millis":240,"deepnote_to_be_reexecuted":false,"cell_id":"08fca461bbbd4711b994fd703f132372","deepnote_cell_type":"code"},"source":"# Normalize data by turning the mean into 0 and scaling to unit variance from each feature\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\ndata_znorm = pd.DataFrame.from_dict(cars_dict,orient='index')\n\nfor i in ['hp','seats','auto']:\n    feature_data = data_znorm[i].values.reshape(-1, 1)\n    scaler.fit(feature_data)\n    data_znorm[i] = scaler.transform(feature_data)\n    \ndata_znorm","block_group":"00012-b1198649-46cc-4f96-b56b-1d269e062b86","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"application/vnd.deepnote.dataframe.v3+json":{"column_count":4,"row_count":5,"columns":[{"name":"hp","dtype":"float64","stats":{"unique_count":5,"nan_count":0,"min":"-0.724650864737854","max":"1.973304643800764","histogram":[{"bin_start":-0.724650864737854,"bin_end":-0.45485531388399225,"count":2},{"bin_start":-0.45485531388399225,"bin_end":-0.1850597630301305,"count":2},{"bin_start":-0.1850597630301305,"bin_end":0.08473578782373126,"count":0},{"bin_start":0.08473578782373126,"bin_end":0.354531338677593,"count":0},{"bin_start":0.354531338677593,"bin_end":0.6243268895314549,"count":0},{"bin_start":0.6243268895314549,"bin_end":0.8941224403853165,"count":0},{"bin_start":0.8941224403853165,"bin_end":1.1639179912391782,"count":0},{"bin_start":1.1639179912391782,"bin_end":1.43371354209304,"count":0},{"bin_start":1.43371354209304,"bin_end":1.703509092946902,"count":0},{"bin_start":1.703509092946902,"bin_end":1.973304643800764,"count":1}]}},{"name":"seats","dtype":"float64","stats":{"unique_count":4,"nan_count":0,"min":"-1.3764944032233708","max":"1.4912022701586514","histogram":[{"bin_start":-1.3764944032233708,"bin_end":-1.0897247358851687,"count":1},{"bin_start":-1.0897247358851687,"bin_end":-0.8029550685469664,"count":0},{"bin_start":-0.8029550685469664,"bin_end":-0.5161854012087642,"count":1},{"bin_start":-0.5161854012087642,"bin_end":-0.22941573387056202,"count":0},{"bin_start":-0.22941573387056202,"bin_end":0.05735393346764006,"count":0},{"bin_start":0.05735393346764006,"bin_end":0.34412360080584237,"count":0},{"bin_start":0.34412360080584237,"bin_end":0.6308932681440447,"count":2},{"bin_start":0.6308932681440447,"bin_end":0.9176629354822468,"count":0},{"bin_start":0.9176629354822468,"bin_end":1.2044326028204488,"count":0},{"bin_start":1.2044326028204488,"bin_end":1.4912022701586514,"count":1}]}},{"name":"auto","dtype":"float64","stats":{"unique_count":2,"nan_count":0,"min":"-0.816496580927726","max":"1.224744871391589","histogram":[{"bin_start":-0.816496580927726,"bin_end":-0.6123724356957946,"count":3},{"bin_start":-0.6123724356957946,"bin_end":-0.4082482904638631,"count":0},{"bin_start":-0.4082482904638631,"bin_end":-0.20412414523193156,"count":0},{"bin_start":-0.20412414523193156,"bin_end":-1.1102230246251565e-16,"count":0},{"bin_start":-1.1102230246251565e-16,"bin_end":0.20412414523193134,"count":0},{"bin_start":0.20412414523193134,"bin_end":0.4082482904638629,"count":0},{"bin_start":0.4082482904638629,"bin_end":0.6123724356957942,"count":0},{"bin_start":0.6123724356957942,"bin_end":0.8164965809277258,"count":0},{"bin_start":0.8164965809277258,"bin_end":1.0206207261596574,"count":0},{"bin_start":1.0206207261596574,"bin_end":1.224744871391589,"count":2}]}},{"name":"cool","dtype":"float64","stats":{"unique_count":2,"nan_count":1,"min":"0.0","max":"1.0","histogram":[{"bin_start":0,"bin_end":0.1,"count":2},{"bin_start":0.1,"bin_end":0.2,"count":0},{"bin_start":0.2,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0},{"bin_start":0.5,"bin_end":0.6000000000000001,"count":0},{"bin_start":0.6000000000000001,"bin_end":0.7000000000000001,"count":0},{"bin_start":0.7000000000000001,"bin_end":0.8,"count":0},{"bin_start":0.8,"bin_end":0.9,"count":0},{"bin_start":0.9,"bin_end":1,"count":2}]}},{"name":"_deepnote_index_column","dtype":"object"}],"rows":[{"hp":-0.724650864737854,"seats":0.3441236008058425,"auto":-0.816496580927726,"cool":0,"_deepnote_index_column":"2008 Honda Civic"},{"hp":-0.3849083192181762,"seats":0.3441236008058425,"auto":-0.816496580927726,"cool":1,"_deepnote_index_column":"2011 Dodge Charger"},{"hp":-0.2590037288197074,"seats":-0.8029550685469664,"auto":1.224744871391589,"cool":1,"_deepnote_index_column":"1985 Chevy Blazer"},{"hp":-0.6047417310250266,"seats":1.4912022701586514,"auto":-0.816496580927726,"cool":0,"_deepnote_index_column":"2001 Honda Odyssey"},{"hp":1.973304643800764,"seats":-1.3764944032233708,"auto":1.224744871391589,"cool":"nan","_deepnote_index_column":"2017 Bugatti Veyron"}]},"text/plain":"                           hp     seats      auto  cool\n2008 Honda Civic    -0.724651  0.344124 -0.816497   0.0\n2011 Dodge Charger  -0.384908  0.344124 -0.816497   1.0\n1985 Chevy Blazer   -0.259004 -0.802955  1.224745   1.0\n2001 Honda Odyssey  -0.604742  1.491202 -0.816497   0.0\n2017 Bugatti Veyron  1.973305 -1.376494  1.224745   NaN","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>hp</th>\n      <th>seats</th>\n      <th>auto</th>\n      <th>cool</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2008 Honda Civic</th>\n      <td>-0.724651</td>\n      <td>0.344124</td>\n      <td>-0.816497</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2011 Dodge Charger</th>\n      <td>-0.384908</td>\n      <td>0.344124</td>\n      <td>-0.816497</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1985 Chevy Blazer</th>\n      <td>-0.259004</td>\n      <td>-0.802955</td>\n      <td>1.224745</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2001 Honda Odyssey</th>\n      <td>-0.604742</td>\n      <td>1.491202</td>\n      <td>-0.816497</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017 Bugatti Veyron</th>\n      <td>1.973305</td>\n      <td>-1.376494</td>\n      <td>1.224745</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"C6dLhGxay9u7","colab_type":"text","cell_id":"41a15973b0724d97b2f685004392f4d1","deepnote_cell_type":"markdown"},"source":"Now that the data is standardized, we can begin building our NNA algorithm. First, we will need a function that **calculates the Euclidean distance between two data points**. For our purposes, we will assume the data points will be stored in arrays of the same length.","block_group":"00013-113062fc-cb1c-4412-9ed4-4c0271d0b8aa"},{"cell_type":"code","metadata":{"id":"2rayoFQnOyvY","colab":{},"colab_type":"code","source_hash":"baf08836","execution_start":1694563748250,"execution_millis":6,"deepnote_to_be_reexecuted":false,"cell_id":"08b2ff479e8f45298660559828e28dbc","deepnote_cell_type":"code"},"source":"# Distance function using formula for Euclidean distance\n\ndef euclidean_dist(datum1, datum2):\n    within_sqrt = 0.0\n    \n    for g in range(datum1.shape[0]):\n        within_sqrt += (datum1[g]- datum2[g]) ** 2\n    \n    distance = np.sqrt(within_sqrt)\n    return(distance)","block_group":"00014-9dc135a7-9832-4ae6-ab41-640d04fd00fb","execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6IaktG5_mUNf","colab_type":"text","cell_id":"b150f1b3c5fc4242a55b78e0b4f57370","deepnote_cell_type":"markdown"},"source":"Next, we calculate the distance between our Bugatti and each of the other cars using the `euclidean_dist` function we defined. For the sake of testing, we do this twice: once for the normalized data `data_mmnorm` and once for the data standardized by sklearn `data_znorm`. Note that we will not input the classification column of cool/uncool to our distance function, which means taking a subset of each array.","block_group":"00015-6556110a-75a1-4997-9a55-402cdf87bafa"},{"cell_type":"code","metadata":{"id":"rY0Wuzz9LZzu","colab":{"height":260,"base_uri":"https://localhost:8080/"},"outputId":"372ac8ee-81b8-42a1-b6e4-aed03bcec62d","colab_type":"code","source_hash":"e7c1f15c","execution_start":1694563749575,"execution_millis":173,"deepnote_to_be_reexecuted":false,"cell_id":"80f42933200c4fb29c6281a29f3a87dc","deepnote_cell_type":"code"},"source":"# FYI: This is how you can call a specific row by name and sub-select features\nbugatti = data.loc[\"2017 Bugatti Veyron\"][[\"hp\",\"seats\",\"auto\"]].values\n\nimport math\n\n# Normalized data by dividing\nprint('Euclidean distances to 2017 Bugatti Veyron (V1)')\nfor car in range(len(data_mmnorm)):\n    d = euclidean_dist(data_mmnorm.iloc[4, :3].values, \n                       data_mmnorm.iloc[car, :3].values)\n    print('  {}: \\t{:01.3f}'.format(data.index[car], d))\n\n# Standardized data\nprint('\\nEuclidean distances to 2017 Bugatti Veyron (V2)')\nfor car in range(len(data_znorm)):\n    d = euclidean_dist(data_znorm.iloc[4, :3].values, \n                       data_znorm.iloc[car, :3].values)\n    print('  {}: \\t{:01.3f}'.format(data.index[car], d))","block_group":"00016-cbea9954-d425-48ad-b67f-1c284fd133e1","execution_count":21,"outputs":[{"name":"stdout","text":"Euclidean distances to 2017 Bugatti Veyron (V1)\n  2008 Honda Civic: \t1.536\n  2011 Dodge Charger: \t1.457\n  1985 Chevy Blazer: \t0.851\n  2001 Honda Odyssey: \t1.707\n  2017 Bugatti Veyron: \t0.000\n\nEuclidean distances to 2017 Bugatti Veyron (V2)\n  2008 Honda Civic: \t3.796\n  2011 Dodge Charger: \t3.562\n  1985 Chevy Blazer: \t2.305\n  2001 Honda Odyssey: \t4.363\n  2017 Bugatti Veyron: \t0.000\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"id":"rHRn4t68nIbB","colab_type":"text","cell_id":"bee99156f9184204b170c6ddb6db6659","deepnote_cell_type":"markdown"},"source":"For the normalized data (with each feature scaled to [0, 1]), we get the following distances:\n\n- Bugatti - Blazer = 0.851\n- Bugatti - Charger = 1.457\n- Bugatti - Civic = 1.536\n- Bugatti - Odyssey = 1.707\n\nFor the standardized data (using sklearn's `StandardScaler`), we get the following distances:\n- Bugatti - Blazer = 2.305\n- Bugatti - Charger = 3.562\n- Bugatti - Civic = 3.796\n- Bugatti - Odyssey = 4.363\n\nNotice that both techniques yielded the same order of cars from nearest to farthest.\n> This is coincidental and unlikely to happen with larger or more varied datasets.\n\nSince the distance between the Bugatti and Chevy Blazer is the smallest, if $k = 1$, we would classify the Bugatti as cool. However, if $k = 4$ there would no longer be a majority, and we would not be able to classify the Bugatti in either category without a tiebreaker protocol.\n\nGenerally speaking, _larger values of $k$ reduce noise but also make the boundaries between classes less distinct_. The best value of $k$ will vary by dataset.","block_group":"00017-a27903c5-0d5d-4884-8d5e-2d5484c01df7"},{"cell_type":"markdown","metadata":{"id":"Vo5iwH6soijh","colab_type":"text","cell_id":"1c50982e7cab494fa00a5db3f8b0d408","deepnote_cell_type":"markdown"},"source":"# Example KNN ","block_group":"00019-ef28c984-dfec-448a-81bd-d97affe4908a"},{"cell_type":"markdown","metadata":{"id":"aF6byhXC7xg0","colab_type":"text","cell_id":"66c09e4aade14f72b4fadc31eb4c9f65","deepnote_cell_type":"markdown"},"source":"Next we will see if we can use KNN on the Pima diabetes dataset.","block_group":"00020-a50e8fea-f990-47e8-9b55-cc78787abf1c"},{"cell_type":"markdown","metadata":{"id":"5jvkEafL70ug","colab_type":"text","cell_id":"ac6e5ca951e343b6858c48e1545924ee","deepnote_cell_type":"markdown"},"source":"## Loading data","block_group":"00021-809d546e-416c-43fc-b04f-e90257c1f027"},{"cell_type":"code","metadata":{"id":"hQAjk52WQ1da","colab":{"height":206,"base_uri":"https://localhost:8080/"},"outputId":"40700207-75b6-40e6-a60c-1d5533796ca0","colab_type":"code","source_hash":"afece580","execution_start":1694563754027,"execution_millis":324,"deepnote_to_be_reexecuted":false,"cell_id":"d06dec8e53ca4d0b8195c6ced26ed29e","deepnote_cell_type":"code"},"source":"url = \"https://raw.githubusercontent.com/MedlyticsUniversal/Data/main/Week1/diabetes.csv\"\nnames = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\ndata = pd.read_csv(url, names=names)\n\n# Dropping NaN rows\ninvalid = ['plas', 'pres', 'skin', 'test', 'mass']\n\nfor i in invalid:\n    data[i].replace(to_replace=0, value=np.nan, inplace=True)\n    \ndata = data.dropna(axis=0).reset_index(drop=True)\n\ndata.head()","block_group":"00022-43e72b41-a553-41bc-88ca-cc3180cb73b7","execution_count":22,"outputs":[{"output_type":"execute_result","execution_count":22,"data":{"application/vnd.deepnote.dataframe.v3+json":{"column_count":9,"row_count":5,"columns":[{"name":"preg","dtype":"int64","stats":{"unique_count":4,"nan_count":0,"min":"0","max":"3","histogram":[{"bin_start":0,"bin_end":0.3,"count":1},{"bin_start":0.3,"bin_end":0.6,"count":0},{"bin_start":0.6,"bin_end":0.8999999999999999,"count":0},{"bin_start":0.8999999999999999,"bin_end":1.2,"count":2},{"bin_start":1.2,"bin_end":1.5,"count":0},{"bin_start":1.5,"bin_end":1.7999999999999998,"count":0},{"bin_start":1.7999999999999998,"bin_end":2.1,"count":1},{"bin_start":2.1,"bin_end":2.4,"count":0},{"bin_start":2.4,"bin_end":2.6999999999999997,"count":0},{"bin_start":2.6999999999999997,"bin_end":3,"count":1}]}},{"name":"plas","dtype":"float64","stats":{"unique_count":5,"nan_count":0,"min":"78.0","max":"197.0","histogram":[{"bin_start":78,"bin_end":89.9,"count":2},{"bin_start":89.9,"bin_end":101.8,"count":0},{"bin_start":101.8,"bin_end":113.7,"count":0},{"bin_start":113.7,"bin_end":125.6,"count":0},{"bin_start":125.6,"bin_end":137.5,"count":1},{"bin_start":137.5,"bin_end":149.4,"count":0},{"bin_start":149.4,"bin_end":161.3,"count":0},{"bin_start":161.3,"bin_end":173.2,"count":0},{"bin_start":173.2,"bin_end":185.10000000000002,"count":0},{"bin_start":185.10000000000002,"bin_end":197,"count":2}]}},{"name":"pres","dtype":"float64","stats":{"unique_count":5,"nan_count":0,"min":"40.0","max":"70.0","histogram":[{"bin_start":40,"bin_end":43,"count":1},{"bin_start":43,"bin_end":46,"count":0},{"bin_start":46,"bin_end":49,"count":0},{"bin_start":49,"bin_end":52,"count":1},{"bin_start":52,"bin_end":55,"count":0},{"bin_start":55,"bin_end":58,"count":0},{"bin_start":58,"bin_end":61,"count":1},{"bin_start":61,"bin_end":64,"count":0},{"bin_start":64,"bin_end":67,"count":1},{"bin_start":67,"bin_end":70,"count":1}]}},{"name":"skin","dtype":"float64","stats":{"unique_count":4,"nan_count":0,"min":"23.0","max":"45.0","histogram":[{"bin_start":23,"bin_end":25.2,"count":2},{"bin_start":25.2,"bin_end":27.4,"count":0},{"bin_start":27.4,"bin_end":29.6,"count":0},{"bin_start":29.6,"bin_end":31.8,"count":0},{"bin_start":31.8,"bin_end":34,"count":1},{"bin_start":34,"bin_end":36.2,"count":1},{"bin_start":36.2,"bin_end":38.400000000000006,"count":0},{"bin_start":38.400000000000006,"bin_end":40.6,"count":0},{"bin_start":40.6,"bin_end":42.8,"count":0},{"bin_start":42.8,"bin_end":45,"count":1}]}},{"name":"test","dtype":"float64","stats":{"unique_count":5,"nan_count":0,"min":"88.0","max":"846.0","histogram":[{"bin_start":88,"bin_end":163.8,"count":2},{"bin_start":163.8,"bin_end":239.6,"count":1},{"bin_start":239.6,"bin_end":315.4,"count":0},{"bin_start":315.4,"bin_end":391.2,"count":0},{"bin_start":391.2,"bin_end":467,"count":0},{"bin_start":467,"bin_end":542.8,"count":0},{"bin_start":542.8,"bin_end":618.6,"count":1},{"bin_start":618.6,"bin_end":694.4,"count":0},{"bin_start":694.4,"bin_end":770.1999999999999,"count":0},{"bin_start":770.1999999999999,"bin_end":846,"count":1}]}},{"name":"mass","dtype":"float64","stats":{"unique_count":5,"nan_count":0,"min":"28.1","max":"43.1","histogram":[{"bin_start":28.1,"bin_end":29.6,"count":1},{"bin_start":29.6,"bin_end":31.1,"count":3},{"bin_start":31.1,"bin_end":32.6,"count":0},{"bin_start":32.6,"bin_end":34.1,"count":0},{"bin_start":34.1,"bin_end":35.6,"count":0},{"bin_start":35.6,"bin_end":37.1,"count":0},{"bin_start":37.1,"bin_end":38.6,"count":0},{"bin_start":38.6,"bin_end":40.1,"count":0},{"bin_start":40.1,"bin_end":41.6,"count":0},{"bin_start":41.6,"bin_end":43.1,"count":1}]}},{"name":"pedi","dtype":"float64","stats":{"unique_count":5,"nan_count":0,"min":"0.158","max":"2.288","histogram":[{"bin_start":0.158,"bin_end":0.371,"count":3},{"bin_start":0.371,"bin_end":0.584,"count":1},{"bin_start":0.584,"bin_end":0.797,"count":0},{"bin_start":0.797,"bin_end":1.01,"count":0},{"bin_start":1.01,"bin_end":1.2229999999999999,"count":0},{"bin_start":1.2229999999999999,"bin_end":1.436,"count":0},{"bin_start":1.436,"bin_end":1.6489999999999998,"count":0},{"bin_start":1.6489999999999998,"bin_end":1.8619999999999999,"count":0},{"bin_start":1.8619999999999999,"bin_end":2.075,"count":0},{"bin_start":2.075,"bin_end":2.288,"count":1}]}},{"name":"age","dtype":"int64","stats":{"unique_count":5,"nan_count":0,"min":"21","max":"59","histogram":[{"bin_start":21,"bin_end":24.8,"count":1},{"bin_start":24.8,"bin_end":28.6,"count":1},{"bin_start":28.6,"bin_end":32.4,"count":0},{"bin_start":32.4,"bin_end":36.2,"count":1},{"bin_start":36.2,"bin_end":40,"count":0},{"bin_start":40,"bin_end":43.8,"count":0},{"bin_start":43.8,"bin_end":47.599999999999994,"count":0},{"bin_start":47.599999999999994,"bin_end":51.4,"count":0},{"bin_start":51.4,"bin_end":55.199999999999996,"count":1},{"bin_start":55.199999999999996,"bin_end":59,"count":1}]}},{"name":"class","dtype":"int64","stats":{"unique_count":2,"nan_count":0,"min":"0","max":"1","histogram":[{"bin_start":0,"bin_end":0.1,"count":1},{"bin_start":0.1,"bin_end":0.2,"count":0},{"bin_start":0.2,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0},{"bin_start":0.5,"bin_end":0.6000000000000001,"count":0},{"bin_start":0.6000000000000001,"bin_end":0.7000000000000001,"count":0},{"bin_start":0.7000000000000001,"bin_end":0.8,"count":0},{"bin_start":0.8,"bin_end":0.9,"count":0},{"bin_start":0.9,"bin_end":1,"count":4}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows":[{"preg":1,"plas":89,"pres":66,"skin":23,"test":94,"mass":28.1,"pedi":0.167,"age":21,"class":0,"_deepnote_index_column":0},{"preg":0,"plas":137,"pres":40,"skin":35,"test":168,"mass":43.1,"pedi":2.288,"age":33,"class":1,"_deepnote_index_column":1},{"preg":3,"plas":78,"pres":50,"skin":32,"test":88,"mass":31,"pedi":0.248,"age":26,"class":1,"_deepnote_index_column":2},{"preg":2,"plas":197,"pres":70,"skin":45,"test":543,"mass":30.5,"pedi":0.158,"age":53,"class":1,"_deepnote_index_column":3},{"preg":1,"plas":189,"pres":60,"skin":23,"test":846,"mass":30.1,"pedi":0.398,"age":59,"class":1,"_deepnote_index_column":4}]},"text/plain":"   preg   plas  pres  skin   test  mass   pedi  age  class\n0     1   89.0  66.0  23.0   94.0  28.1  0.167   21      0\n1     0  137.0  40.0  35.0  168.0  43.1  2.288   33      1\n2     3   78.0  50.0  32.0   88.0  31.0  0.248   26      1\n3     2  197.0  70.0  45.0  543.0  30.5  0.158   53      1\n4     1  189.0  60.0  23.0  846.0  30.1  0.398   59      1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>preg</th>\n      <th>plas</th>\n      <th>pres</th>\n      <th>skin</th>\n      <th>test</th>\n      <th>mass</th>\n      <th>pedi</th>\n      <th>age</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>89.0</td>\n      <td>66.0</td>\n      <td>23.0</td>\n      <td>94.0</td>\n      <td>28.1</td>\n      <td>0.167</td>\n      <td>21</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>137.0</td>\n      <td>40.0</td>\n      <td>35.0</td>\n      <td>168.0</td>\n      <td>43.1</td>\n      <td>2.288</td>\n      <td>33</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>78.0</td>\n      <td>50.0</td>\n      <td>32.0</td>\n      <td>88.0</td>\n      <td>31.0</td>\n      <td>0.248</td>\n      <td>26</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>197.0</td>\n      <td>70.0</td>\n      <td>45.0</td>\n      <td>543.0</td>\n      <td>30.5</td>\n      <td>0.158</td>\n      <td>53</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>189.0</td>\n      <td>60.0</td>\n      <td>23.0</td>\n      <td>846.0</td>\n      <td>30.1</td>\n      <td>0.398</td>\n      <td>59</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"lO_knzAAe93n","colab_type":"text","cell_id":"80bf7465169f4b4fa17a51fa5f34ba8e","deepnote_cell_type":"markdown"},"source":"## Splitting data into training, validation, and testing sets","block_group":"00023-fc7331f4-8ae1-48e6-b413-b5c6d8752e76"},{"cell_type":"code","metadata":{"id":"b8pRTCC3PatJ","colab":{},"colab_type":"code","source_hash":"7a578b6e","execution_start":1694563755960,"execution_millis":12,"deepnote_to_be_reexecuted":false,"cell_id":"90be05db82454c77a77f899144f7c87a","deepnote_cell_type":"code"},"source":"from sklearn.model_selection import train_test_split\n\n# Columns we will use to make predictions with (features!). Feel free to play around with these:\nX_cols = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age']\n\n# Column that we want to predict (the labels)\ny_col = 'class'\n\n# 80-20 train-test split of datset\ntest_size = 0.2\nX_train, X_test, y_train, y_test = train_test_split(data[X_cols], data[y_col], test_size=test_size, random_state=0)\n\n# Further split X and y of training into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=test_size, random_state=0)","block_group":"00024-84df3aba-21a9-4a64-9fec-92b247af856c","execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qrnVJ8Ovy9vT","colab_type":"text","cell_id":"ca36fbf46c024322a18db1e03b1dd480","deepnote_cell_type":"markdown"},"source":"## Normalizing data","block_group":"00025-f3075a02-087b-4988-b150-ff6ab74140de"},{"cell_type":"code","metadata":{"id":"Lt29VKMJy9vT","colab":{},"colab_type":"code","source_hash":"f5abb80d","execution_start":1694563758492,"execution_millis":20,"deepnote_to_be_reexecuted":false,"cell_id":"7daa4efaad7d46e393b6cf42eef00187","deepnote_cell_type":"code"},"source":"scaler = StandardScaler()\n\nfor i in list(X_train):\n    feature_data_train = X_train[i].values.reshape(-1, 1)\n    scaler.fit(feature_data_train)\n    X_train[i] = scaler.transform(feature_data_train)\n\nfor j in list(X_test):\n    feature_data_test = X_test[j].values.reshape(-1, 1)\n    scaler.fit(feature_data_test)\n    X_test[j] = scaler.transform(feature_data_test)\n    \nfor k in list(X_val):\n    feature_data_val = X_val[k].values.reshape(-1, 1)\n    scaler.fit(feature_data_val)\n    X_val[k] = scaler.transform(feature_data_val)","block_group":"00026-77bda6c1-5ef2-44dd-93ec-cefc388689e6","execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7-2ZM8AGIe0S","colab_type":"text","cell_id":"d7a22c2a8037454298ee0c5670954c71","deepnote_cell_type":"markdown"},"source":"## Using sklearn's KNN\n\nLuckily for us, sklearn has some quick and easy functions for normalizing the data, finding Euclidean distances, training, and testing with [KNN](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html). Try `k=5` to start.","block_group":"00027-fc28d2c5-3014-400d-95ce-f9e3ebbec316"},{"cell_type":"code","metadata":{"id":"R0FsKkp_qw-x","colab":{"height":73,"base_uri":"https://localhost:8080/"},"outputId":"30c9d111-3951-4031-d7a4-9a75618b4fb5","colab_type":"code","source_hash":"689f6f1d","execution_start":1694563760804,"execution_millis":304,"deepnote_to_be_reexecuted":false,"cell_id":"1db63b3db0f242c1856807930bf8dbaa","deepnote_cell_type":"code"},"source":"from sklearn.neighbors import KNeighborsClassifier\n\n# Creating model with sklearn's KNeighborsClassifier — after running these cells, play around with the parameter n!\nknn = KNeighborsClassifier(n_neighbors=5)\n\n# Training/fitting a model with training data\nknn.fit(X_train, y_train)","block_group":"00028-c32df28e-ae60-4488-b157-856a819ada76","execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"KNeighborsClassifier()","text/html":"<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier()</pre></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"J8y1RuAoy9vr","colab":{"height":73,"base_uri":"https://localhost:8080/"},"outputId":"4221ba2a-2497-4de4-9019-566ad130d51d","colab_type":"code","source_hash":"447fcafb","execution_start":1694563765304,"execution_millis":46,"deepnote_to_be_reexecuted":false,"cell_id":"3fd7b7b031004da6907c4f1b4636086f","deepnote_cell_type":"code"},"source":"from sklearn.metrics import accuracy_score\nimport time\n\ny_train_pred=knn.predict(X_train)\nstart = time.time()\npredictions_fast = knn.predict(X_val)\n\nprint('Took {} seconds'.format(time.time() - start))\nprint(\"Training Accuracy is \", accuracy_score(y_train, y_train_pred)*100)\nprint(\"Validation Accuracy is \", accuracy_score(y_val,predictions_fast)*100)","block_group":"00029-f0c60256-ddb3-4e57-a83a-0aa4f1c60313","execution_count":26,"outputs":[{"name":"stdout","text":"Took 0.007447957992553711 seconds\nTraining Accuracy is  79.2\nValidation Accuracy is  80.95238095238095\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"id":"w-hHcKgsy9vw","colab_type":"text","cell_id":"b6d6e22313574fe695ed6b402c447089","deepnote_cell_type":"markdown"},"source":"# Conclusions and Limitations\n\nKNN provides a good baseline, machine learning classifier that is conceptually intuitive and easy to implement. One major advantage is that, by normalizing input data, a KNN algorithm can combine any number of features regardless of their original scales.\n\nHowever, KNN can run into problems with more complex data sets:\n- With additional dimensions, it can be harder to define meaningful distances.\n- The testing phase can slow down significantly with large datasets since each point's distance is measured to every other point.\n- \"Majority votes\" may be skewed if one classification is significantly more common than the others.\n- There is no consideration for correlated features.\n\nAs a final note, there are other ways to instantiate KNNs, and other types of NNA beyond k-Nearest. For example, to solve the large majority problem, we could have used a **weighted voting system** where nearer neighbors' votes carry more weight.","block_group":"00030-03821ffd-1afc-40eb-b9b8-2e2608e0e92b"},{"cell_type":"markdown","metadata":{"cell_id":"33562559e2b34665b1f505abea470494","deepnote_cell_type":"markdown"},"source":"_Note: The tutorials and graphs on normalization methods utilize content from [Codecademy](https://www.codecademy.com/article/normalization)._","block_group":"d7a4a7b535b84295adcc7dafe4bd953e"},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=67231ff1-13ba-4f0b-99c2-0fe76b4d3844' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NearestNeighbors_Tutorial","provenance":[],"collapsed_sections":[]},"deepnote":{},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python"},"deepnote_notebook_id":"9a824254b14c410a8f1bc2ed238c931b","deepnote_execution_queue":[],"deepnote_persisted_session":{"createdAt":"2023-09-10T04:01:12.189Z"}}}